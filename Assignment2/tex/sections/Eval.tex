\section{Evaluation Metrics}
The accuracy different models used to fit the above data is evaluated using different metrics which are stated as follows:

\begin{itemize}
  \item \textbf{\hypertarget{acc_def}{Accuracy}:} This is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations
  \[
    \text{Accuracy} = \frac{\text{Total Number of Correct Predictions}}{\text{Total Number of Predictions}}
  \]

  \item \textbf{Precision (Macro Average):} Precision is the ratio of correctly predicted positive observations to the total predicted positive observations
  \[
    \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
  \]

  \item \textbf{Recall (Macro Average):} Recall (also known as sensitivity) is the ratio of correctly predicted positive observations to all observations in actual class
  \[
    \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
  \]

  \item \textbf{F1 Score (Macro Average):} The F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account
  \[
    \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
  \]
\end{itemize}